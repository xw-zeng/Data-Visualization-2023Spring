{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef2012e-7f34-4e71-8e44-ef70bac3de01",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "\n",
    "**Goal:** Transform raw text input into *normalized* sequence of\n",
    "*tokens*. Prepare for *feature extraction*.\n",
    "\n",
    "\"<span style=\"color:red\">Hi</span>. This is an <span\n",
    "style=\"color:green\">example</span> <span\n",
    "style=\"color:blue\">sentence</span> in an <span\n",
    "style=\"color:green\">Example</span> <span\n",
    "style=\"color:purple\">Document</span>.\" → \\[<span\n",
    "style=\"color:red\">hi</span>, <span style=\"color:green\">example</span>,\n",
    "<span style=\"color:blue\">sentence</span>, <span\n",
    "style=\"color:green\">example</span>, <span\n",
    "style=\"color:purple\">document</span>\\]\n",
    "\n",
    "Text processing includes many steps and hence many decisions that have\n",
    "**big effect** on your results. Several *possibilities* will be shown\n",
    "here. If and how to apply them depends heavily on your data and your\n",
    "later analysis.\n",
    "\n",
    "## The document corpus\n",
    "\n",
    "A *corpus* contains the *documents* that we want to process. Each\n",
    "document can be accessed by a unique *document label* or *document ID*.\n",
    "The document itself is usually a (very long) character string (Python\n",
    "type: *str*) that may contain line breaks.\n",
    "\n",
    "You normally load a corpus from files, a database or other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc912337-3514-4e3f-9fa0-a42089958e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small toy corpus with some news headlines \n",
    "corpus = {   # document label: document text\n",
    "    'doc1': \"This is Andrew's text, isn't it?\",\n",
    "    'doc2': \"feet cats wolves talked\",\n",
    "    'doc3': \"据报到，复旦大学启动校园准封闭管理\",    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28d7a72-d643-448b-b6c1-425ad2dc924d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is Andrew's text, isn't it?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access by document label\n",
    "corpus['doc1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a282f489-8c70-4e30-b1f0-cee4d4203af0",
   "metadata": {},
   "source": [
    "## STEP 1: Tokenization（分词）\n",
    "\n",
    "**Goal:** Break down document text into smaller, meaningful components\n",
    "(paragraphs, sentences, **words**) → from a document, form a list of\n",
    "*tokens*\n",
    "\n",
    "In our case: We apply *word tokenization*, so **token = word**\n",
    "\n",
    "With plain Python: calling `split()` on a string splits it by\n",
    "*whitespace*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8cc93b3-8b11-4ac5-acd3-da29b6f2f067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', \"Andrew's\", 'text,', \"isn't\", 'it?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "corpus['doc1'].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf837c97-56ed-4de5-bd1e-d047374ceadb",
   "metadata": {},
   "source": [
    "**Tokenization is not trivial.**\n",
    "\n",
    "-   how to handle punctuation, quotes, hyphens?\n",
    "-   how to handle contractions? (\"don't\" or \"is't\")\n",
    "\n",
    "→ depends on your text (language, source/medium)\n",
    "\n",
    "-   `str.split()` might not be optimal\n",
    "-   [NLTK](http://www.nltk.org/) implements several word- and sentence\n",
    "    tokenizers, e.g.:\n",
    "    -   [WordPunctTokenizer](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.WordPunctTokenizer):\n",
    "        punctuation become separate tokens\n",
    "    -   [TreebankWordTokenizer](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.treebank.TreebankWordTokenizer):\n",
    "        Default tokenizer → come up with a set of rules\n",
    "    -   [RegExpTokenizer](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.regexp):\n",
    "        Define your own tokenizer with [Regular\n",
    "        Expression](https://en.wikipedia.org/wiki/Regular_expression)\n",
    "        rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d59838-4fb7-4ed6-8bb1-e4f45cb2515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, jieba\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cdca0d-b81f-4356-a691-222afa0e77ea",
   "metadata": {},
   "source": [
    "With nltk.tokenize.wordpunct_tokenize():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a289c0c7-dd9a-47e9-acf5-f24746c9dbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Andrew', \"'\", 's', 'text', ',', 'isn', \"'\", 't', 'it', '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO:\n",
    "nltk.tokenize.wordpunct_tokenize(corpus['doc1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9942ac-00bb-4890-b958-b7ffc79ae2cc",
   "metadata": {},
   "source": [
    "With nltk.word_tokenize():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ed08a20-1321-469c-aa37-e3f0cd5599c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Andrew', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_tokenize uses TreebankWordTokenizer by default\n",
    "# TODO:\n",
    "nltk.tokenize.word_tokenize(corpus['doc1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9796c-bbe6-452e-9f2c-91e16e8540d0",
   "metadata": {},
   "source": [
    "**中文分词**\n",
    "\n",
    "-   [jieba](https://github.com/fxsjy/jieba) 中文分词，jieba.cut()接受如下输入参数: \n",
    "    - 需要分词的字符串\n",
    "    - cut_all 参数用来控制是否采用全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef275dd9-03c8-4978-82df-7b8e2c16dea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['据', '报到', '，', '复旦大学', '启动', '校园', '准', '封闭', '管理']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "jieba.lcut(corpus['doc3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa768d-6c5e-41f8-a6b1-f1780b0a5ae0",
   "metadata": {},
   "source": [
    "**tokenize whole corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54edfa91-6936-4d06-98d9-ec1dfc56e127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['据', '报到', '，', '复旦', '复旦大学', '大学', '启动', '校园', '准', '封闭', '闭管', '管理']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "jieba.lcut(corpus['doc3'], cut_all = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b586fbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc1': ['This', 'is', 'Andrew', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?'],\n",
       " 'doc2': ['feet', 'cats', 'wolves', 'talked'],\n",
       " 'doc3': ['据', '报到', '，', '复旦大学', '启动', '校园', '准', '封闭', '管理']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = {doc_label: nltk.word_tokenize(text) if doc_label != 'doc3'\n",
    "          else jieba.lcut(text)\n",
    "          for doc_label, text in corpus.items()}\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f16d7b-9367-432a-ae45-6ab053ea1dee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## STEP 2: Text normalization（词形归一化）\n",
    "\n",
    "## Stemming or Lemmatization\n",
    "\n",
    "**Goal:** Reduce inflected words to a common form so that they're\n",
    "counted as one.\n",
    "\n",
    "### Stemming\n",
    "\n",
    "Remove affixes from a word to get base form *(stem)* of a word → stem\n",
    "might not be a lexicographically correct word\n",
    "\n",
    "-   books → book\n",
    "-   booked → book\n",
    "-   **employees → employ**\n",
    "-   **argued → argu**\n",
    "\n",
    "NLTK implements several stemming algorithms, e.g.,\n",
    "-   [PorterStemmer](https://www.nltk.org/howto/stem.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1451694d-bfe6-4c10-bf16-78de6fcb6bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feet cats wolves talked'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['doc2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0193597f-8650-47e9-9407-7c32411967fd",
   "metadata": {},
   "source": [
    "With stem() method in nltk.stem.PorterStemmer() class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb08dc11-a3d8-41cd-941b-2c6a7d8face2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feet', 'cat', 'wolv', 'talk']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "[stemmer.stem(token) for token in tokens['doc2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e642ef-0088-40a7-a126-f875fe549a21",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Find *lemma* (dictionary form) of a inflected word → a lemma is always a\n",
    "lexicographically correct word\n",
    "\n",
    "Implemented for English in NLTK with `WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "899115b2-8070-4a2d-b7dd-5c03611c4f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/kirito/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "320738eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/kirito/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3916e01-0193-4f73-9da3-1fb4048bf398",
   "metadata": {},
   "source": [
    "With lemmatize() method in nltk.stem.WordNetLemmatizer() class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b2ff697-68e5-4759-8164-2a00f6b11869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foot', 'cat', 'wolf', 'talked']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "[lemmatizer.lemmatize(token) for token in tokens['doc2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830bbb4-bdef-4302-a647-eb297bccb9dc",
   "metadata": {},
   "source": [
    "### Normalizing captical letters\n",
    "Usually: convert all words to lowercase.\n",
    "\n",
    "**Can be problematic because of \"capitonyms\":**\n",
    "\n",
    "-   e.g. in English: \"May\" ≠ \"may\", \"Pole\" ≠ \"pole\"\n",
    "\n",
    "Methods in Python: `str.lower()`, `str.upper()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "207c25ee-c099-4053-9adf-a67d927f3334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'andrew', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO\n",
    "[t.lower() for t in tokens[\"doc1\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e47c1-9c44-4d97-b49a-f713bff5d4e7",
   "metadata": {},
   "source": [
    "### Normalizing whole tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7945748d-27a2-41f9-b0f8-18197d64d909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc1': ['this', 'is', 'andrew', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?'],\n",
       " 'doc2': ['foot', 'cat', 'wolf', 'talked'],\n",
       " 'doc3': ['据', '报到', '，', '复旦大学', '启动', '校园', '准', '封闭', '管理']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "normtokens = {doc_label: [lemmatizer.lemmatize(t).lower() for t in tokenlist]\n",
    "    for doc_label, tokenlist in tokens.items()\n",
    "}\n",
    "normtokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a28ad-33df-4d09-8a9e-3613d0ba0190",
   "metadata": {
    "tags": []
   },
   "source": [
    "## STEP 3: Removing stopwords（删除停用词）\n",
    "\n",
    "*Stopwords* are words that are removed before doing further text\n",
    "analysis. \n",
    "\n",
    "Usually: Very common words for a certain language that\n",
    "transport little information.\n",
    "\n",
    "Stopword list depends on:\n",
    "\n",
    "-   language\n",
    "-   your data / research scenario (filter out too common words)\n",
    "-   later text analysis method, e.g.:\n",
    "    -   *tf-idf* automatically reduces importance of very common words\n",
    "        (as opposed to *Bag-of-Words*)\n",
    "    -   sentiment analysis: bad idea to have words like \"not\" in the\n",
    "        stopword list!\n",
    "\n",
    "NLTK has a list of stopwords for some languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3df0b4b6-4c34-4eb5-bfed-52677a84cb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kirito/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a5e1e10-cd64-4a58-bb0c-3934154b4ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c5653ff-9791-40fd-9aad-3cce37fc7c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_english[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa7851d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ea95a-795b-4d46-aaf7-da49c8f136af",
   "metadata": {},
   "source": [
    "**中文停用词**\n",
    "\n",
    "Load the chinese stopwords from the file \"stop_words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2e0dcf4-6fdb-4a31-833e-252f69c4b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "with open('stop_words') as f:\n",
    "    stopwords_chinese = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a22e08c-00bd-48ca-acce-118e7ef659c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['他', '以', '们', '任', '会']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "stopwords_chinese[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6900bc4f-666a-4b0b-9dda-8e236c8ed015",
   "metadata": {},
   "source": [
    "**Remove english and chinese stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd7f55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "stopwords = stopwords_chinese + stopwords_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4570184",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoptokens = {doc_label: [t for t in tokenlist if t not in stopwords]\n",
    "    for doc_label, tokenlist in normtokens.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "78994d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc1': ['andrew', \"'s\", 'text', \"n't\"],\n",
       " 'doc2': ['foot', 'cat', 'wolf', 'talked'],\n",
       " 'doc3': ['报到', '复旦大学', '启动', '校园', '准', '封闭', '管理']}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoptokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
